# ADR Code Synth - Environment Configuration Template
# Copy this file to .env and fill in your API keys

# ============================================================================
# LLM PROVIDER SELECTION
# ============================================================================
# Choose which LLM provider to use: "openai", "groq", or "gemini"
# Default: openai (backward compatible)
LLM_PROVIDER=openai

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Available models: gpt-4o, gpt-4.1-mini, gpt-4.1-preview, gpt-4-turbo
# Default: gpt-4.1-mini
OPENAI_MODEL=gpt-4.1-mini

# Optional: Custom OpenAI endpoint (e.g., for Azure OpenAI or local models)
# OPENAI_BASE_URL=https://api.openai.com/v1

# ============================================================================
# GROQ CONFIGURATION
# ============================================================================
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=gsk-your-groq-api-key-here

# Available models (all extremely fast):
# - llama3-70b-8192: Best quality, ~100 tokens/sec
# - mixtral-8x7b-32768: Good quality, ~150 tokens/sec
# - gemma-7b-it: Lightweight, ~200 tokens/sec
# Default: llama3-70b-8192
GROQ_MODEL=llama3-70b-8192

# ============================================================================
# GEMINI CONFIGURATION
# ============================================================================
# Get your API key from: https://console.cloud.google.com/apis/credentials
GOOGLE_API_KEY=AIza-your-gemini-api-key-here

# Available models:
# - gemini-1.5-pro: Best quality, 128K context window
# - gemini-1.5-flash: Faster, 1M context window (recommended for large files)
# Default: gemini-1.5-pro
GEMINI_MODEL=gemini-1.5-pro

# ============================================================================
# COMMON LLM PARAMETERS (shared across all providers)
# ============================================================================
# Temperature: Controls randomness in model output
# - 0.0: Most deterministic, focused responses
# - 0.7: Balanced creativity and coherence
# - 1.0: More creative, varied responses
# Default: 0.1 (low temperature for structured analysis)
TEMPERATURE=0.1

# Max Tokens: Maximum number of tokens in the LLM response
# Leave empty for model default
# Typical values: 1000-4000 for analysis tasks
MAX_TOKENS=

# ============================================================================
# ADDITIONAL CONFIGURATION
# ============================================================================
# Project-specific settings are managed in project-config.yaml files
# See project-inputs/[project-name]/project-config.yaml

# Example project configuration:
# llm:
#   provider: "groq"  # Override global provider for this project
#   model: "llama3-70b-8192"
#   temperature: 0.2
#   max_tokens: 2000

# ============================================================================
# NOTES
# ============================================================================
# 1. Only configure the provider you're using
# 2. OpenAI API key is required if LLM_PROVIDER=openai
# 3. Groq API key is required if LLM_PROVIDER=groq
# 4. Gemini API key is required if LLM_PROVIDER=gemini
# 5. You can switch providers by changing LLM_PROVIDER and the corresponding API key
# 6. Project configs can override global settings

# ============================================================================
# USAGE EXAMPLES
# ============================================================================
# Using OpenAI (default):
#   LLM_PROVIDER=openai
#   OPENAI_API_KEY=sk-...
#   OPENAI_MODEL=gpt-4o

# Using Groq (faster inference):
#   LLM_PROVIDER=groq
#   GROQ_API_KEY=gsk-...
#   GROQ_MODEL=llama3-70b-8192

# Using Groq with faster model:
#   LLM_PROVIDER=groq
#   GROQ_API_KEY=gsk-...
#   GROQ_MODEL=mixtral-8x7b-32768

# Using Gemini (large context windows):
#   LLM_PROVIDER=gemini
#   GOOGLE_API_KEY=AIza-...
#   GEMINI_MODEL=gemini-1.5-pro

# Using Gemini Flash for large codebases:
#   LLM_PROVIDER=gemini
#   GOOGLE_API_KEY=AIza-...
#   GEMINI_MODEL=gemini-1.5-flash